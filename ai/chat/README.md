# ğŸ¦™ Ollama CLI

Una interfaz de lÃ­nea de comandos (CLI) interactiva para chatear con Ollama, construida con TypeScript y Node.js.

## ğŸ“‹ CaracterÃ­sticas

- ğŸ’¬ **Chat interactivo** con modelos de Ollama
- ğŸ”„ **Cambio dinÃ¡mico de modelos** durante la conversaciÃ³n
- ğŸ“ **GestiÃ³n de historial** de conversaciones
- ğŸ¨ **Interfaz colorida** y fÃ¡cil de usar
- âš¡ **Comandos Ãºtiles** para gestionar modelos
- ğŸ”§ **ConfiguraciÃ³n flexible** de parÃ¡metros

## ğŸš€ InstalaciÃ³n

### Prerrequisitos

1. **Node.js** (versiÃ³n 18 o superior)
2. **Ollama** instalado y ejecutÃ¡ndose

### Instalar Ollama

```bash
# En macOS/Linux
curl -fsSL https://ollama.com/install.sh | sh

# En Windows
# Descargar desde https://ollama.com/download
```

### Instalar el CLI

```bash
# Clonar el repositorio (o navegar al directorio)
cd ai/chat

# Instalar dependencias
npm install

# Compilar el proyecto
npm run build

# Instalar globalmente (opcional)
npm link
```

## ğŸ¯ Uso

### Comandos Disponibles

#### 1. Chat Interactivo

```bash
# Chat bÃ¡sico
ollama-cli chat

# Chat con modelo especÃ­fico
ollama-cli chat -m llama2

# Chat con configuraciÃ³n personalizada
ollama-cli chat -m codellama -t 0.5 -u http://localhost:11434
```

**Opciones para el comando `chat`:**
- `-m, --model <model>`: Modelo a utilizar (por defecto: llama2)
- `-t, --temperature <temp>`: Temperatura para la generaciÃ³n 0.0-1.0 (por defecto: 0.7)
- `-u, --url <url>`: URL base de Ollama (por defecto: http://localhost:11434)

#### 2. Verificar Estado

```bash
# Verificar conexiÃ³n con Ollama
ollama-cli health

# Con URL personalizada
ollama-cli health -u http://localhost:11434
```

### Comandos del Chat Interactivo

Dentro del chat, puedes usar los siguientes comandos especiales:

- `/help` - Muestra la ayuda
- `/clear` - Limpia el historial de conversaciÃ³n
- `/history` - Muestra el historial completo
- `/model` - Cambiar el modelo actual
- `/exit` - Salir del chat

## ğŸ“‚ Estructura del Proyecto

```
ai/chat/
â”œâ”€â”€ package.json          # ConfiguraciÃ³n del proyecto
â”œâ”€â”€ tsconfig.json         # ConfiguraciÃ³n de TypeScript
â”œâ”€â”€ README.md            # Este archivo
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts         # Punto de entrada principal
â”‚   â”œâ”€â”€ types/
â”‚   â”‚   â””â”€â”€ index.ts     # Definiciones de tipos
â”‚   â”œâ”€â”€ chat/
â”‚   â”‚   â”œâ”€â”€ OllamaClient.ts   # Cliente principal de Ollama
â”‚   â”‚   â””â”€â”€ ChatManager.ts    # Gestor de chat interactivo
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ ollama-api.ts     # Utilidades de API REST
â””â”€â”€ dist/                # Archivos compilados (generado)
```

## ğŸ”§ Desarrollo

### Scripts Disponibles

```bash
# Desarrollo con recarga automÃ¡tica
npm run dev

# Compilar para producciÃ³n
npm run build

# Ejecutar versiÃ³n compilada
npm start

# Ejecutar directamente con ts-node
npx ts-node src/index.ts chat
```

### Modo Desarrollo

```bash
# Ejecutar en modo desarrollo
npm run dev chat -m llama2

# O directamente con ts-node
npx ts-node src/index.ts chat -m llama2
```

## ğŸ“š Ejemplos de Uso

### Ejemplo 1: Chat BÃ¡sico

```bash
$ ollama-cli chat
ğŸ¦™ Bienvenido al CLI de Ollama
Escribe '/help' para ver los comandos disponibles
Escribe '/exit' para salir

âœ” Conectado exitosamente. Modelo actual: llama2

? TÃº: Hola, Â¿cÃ³mo estÃ¡s?
ğŸ¦™ llama2: Â¡Hola! Estoy muy bien, gracias por preguntar. Soy una IA asistente y estoy aquÃ­ para ayudarte con cualquier pregunta o tarea que tengas. Â¿En quÃ© puedo ayudarte hoy?

? TÃº: /model
? Ingresa el nombre del nuevo modelo: codellama
âœ” Modelo cambiado exitosamente a: codellama

? TÃº: /exit
Â¡Hasta luego!
```

### Ejemplo 2: Verificar Estado

```bash
$ ollama-cli health
âœ” Ollama estÃ¡ funcionando correctamente en http://localhost:11434
âœ… El servidor estÃ¡ listo para recibir mensajes
```

## ğŸ› ï¸ ConfiguraciÃ³n Avanzada

### Variables de Entorno

Puedes configurar el CLI usando variables de entorno:

```bash
export OLLAMA_HOST=http://localhost:11434
export OLLAMA_DEFAULT_MODEL=llama2
export OLLAMA_TEMPERATURE=0.7
```

### ConfiguraciÃ³n de Modelos

El CLI soporta todos los modelos disponibles en Ollama:

- **Modelos de propÃ³sito general**: llama2, mistral, phi
- **Modelos de cÃ³digo**: codellama, deepseek-coder
- **Modelos matemÃ¡ticos**: deepseek-math
- **Y muchos mÃ¡s...**

## ğŸ› SoluciÃ³n de Problemas

### Error: "Cannot find module"

```bash
# Reinstalar dependencias
rm -rf node_modules package-lock.json
npm install
npm run build
```

### Error: "Cannot connect to Ollama"

```bash
# Verificar que Ollama estÃ© ejecutÃ¡ndose
ollama serve

# Verificar conexiÃ³n
ollama-cli health
```

### Error: "Model not found"

```bash
# Verificar que el modelo estÃ© disponible en Ollama
ollama list

# Descargar el modelo necesario directamente con Ollama
ollama pull llama2
```

## ğŸ¤ Contribuir

1. Fork el repositorio
2. Crea una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -am 'Agregar nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia ISC.

## ğŸ™ Agradecimientos

- [Ollama](https://ollama.com/) por la plataforma de LLMs
- [Commander.js](https://github.com/tj/commander.js/) por el framework CLI
- [Inquirer.js](https://github.com/SBoudrias/Inquirer.js/) por las prompts interactivas
- [Chalk](https://github.com/chalk/chalk) por los colores en terminal
- [Ora](https://github.com/sindresorhus/ora) por los spinners de carga

---

Â¡Disfruta chateando con Ollama! ğŸš€